<!-- SPDX-License-Identifier: zlib-acknowledgement -->

assignment questions similar to problem set

problem set solution format to justify correctness of algorithm

“Create an array of size n where each element stores (x,y) where x represents blah and y represents blah”

is expected time complexity ok?

revisit discord course chat
TODO: Use self balancing BST such as red-black tree as has logn search and insertion over hashmap


TODO: use problem set solutions as formats for proofs

# Assignment 1
## Question 1
TODO: possible use of min-heap?

correctness:
1st, Why my algorithm does not count extra pairs that it shouldn't count.
2nd, Why my algorithm does not miss pairs that it should have counted.
Therefore my algorithm counts correctly.

1.1
  TODO: if A[j] = A[i] continue
**Algorithm**
* Sort array A using quick sort
* Initialise a left and right index pointer into array A.
  Let left index pointer i = 0
  Let right index pointer j = len(A) - 1
* Iterate over array A until i = j 
  With current index values, evaluate the expression 2A[i] - 3A[j] = k
  If k = x, then distinct indices found
  If k < x, then increment i. As A sorted, incrementing i will increase term 2A[i], thereby increasing k. 
  If k > x, then increment j. As A sorted, incrementing j will increase term 3A[j], thereby decreasing k. 
* If reach condition i = j, then no distinct indices found.
**Proof**
* Terminates: Each progression of the loop over A results in the convergence of i and j. 
              As len(A) is finite, clearly i will equal j if no indices found
* Correctness: A contains distinct elements, and exit loop if i = j. Therefore, any solution will satisfy 1 < i, j < n 
* Time Complexity: Quick sort of A takes O(nlogn) time.
 Sorting array A using quicksort is O(nlogn)
 Finding length of array A to initialise right index pointer is O(n)
 Iterating over array A with left and right index pointers is O(n/2)
 Therefore, O(nlogn) + O(n) + O(n/2) = O(nlogn) 



1.2
**Algorithm**
* Populate a hashmap H with A, such that H maps values of A to their indexes in A
* Iterate over A, maintaining current index in variable i
  For the current value in A, A[i], compute the other value that must exist, A[j] to satisfy condition.
  I.e. Compute A[j] = (2A[j] - x) / 3
  TODO: if A[j] = A[i] continue
  Check if this value of A[j] exists in hashmap H
  If it exists, let its value, i.e. its index, be j 
  If j != i then found distinct indices 
* If complete iteration over A, then no distinct indices found.
**Correctness**

**Time Complexity**
* Populating hashmap H is O(n)
* Iterating over array A is O(n)
* Checking element in hashmap H is expected O(1)
* Therefore, O(n) + O(n)·O(1) = O(n)

1.3
TODO: remember, distinct and that i < j!
TODO: perhaps this is where master theorem used?
**Algorithm**
index_value_pairs()
sort(index_value_pairs)

self_balancing_pairs=()
for i in A:
  required_val = (x - A[i])
  pair = bin_search(required_val, index_value_pairs)
  j = pair.index
  if i < j && A[i] > A[j]: 
    search(i, self_balancing_pairs) as distinct, only one other pair possible
    no check necessary as i < j?
    add_pair(i, j)
    
**Correctness**
mention as positive and distinct
**Time Complexity**


## Question 2
2.1
**Algorithm**
\item Sort A with quicksort
\item Iterate over B, maintaining a running count of how many shared cards 
\item For each card in B, check if it exists in A via binary search
  If card exists, increment shared card count
\item Once iteration finished, the number of cards owned by Blake but not Red is (n - shared-card-count)
**Correctness**

**Time Complexity**

2.2 
**Time Complexity**
* As we are iterating over k students, we know the outer loop is O(k)
* The merge operation is O(S).
  Consider worst case with deck d1 and deck d2, where d1 + d2 = S
  If all cards in d1 are dissimilar to cards in d2, then have to loop over every card in S. 
* Therefore, O(k)·O(S) = O(S·k)
* As, k > logk, O(S·k) > O(S·logk)
  I.e. merging is growing linearly as opposed to logarithmically

Think how many times he needs to merge N* and Ni arrays , in worst case! What is the size of the N* array finally（in worst case）?

2.3.
**Algorithm**
  \item Divide array N into two subarrays of approximately equal parts, i.e. equal number of collections in each subarray
  \item Recursively apply this division on each subarray, until subarray contains only one collection
  \item This is the base case, as each collection contains distinct cards 
  \item Working way up call stack, merge subarrays, i.e. collections, until all collections are merged
  \item The resultant collection contains distinct cards for Gerald's class
**Correctness**
 * By problem definition, merging two collections will always produce a collection of distinct cards.
 * So, if distinct at height log(k), then also distinct at height log(k - 1), etc.
**Time Complexity**
* The depth of the call stack tree resulting from dividing N is O(logk), as there are k collections
  In otherwords, the number of collections to be merged grows logarithmically
* As explained in question 2.2, the merge is O(S)
* Therefore O(logk)O(S) = O(Slogk)

## Question 3
1.
a. lim∞ f'(x)/g'(x) is 0. 
f(n) grows fastest
Limit asymptotic theorem gives f(n) = O(g(n)) 
b. g(n) grows fastest  
c. f(n) grows faster but also shrinks faster? 

In asymptotic notation constant coefficients and constant values can be ignored

Try look at section 3 of problem set 1
TODO: https://edstem.org/au/courses/11846/discussion/1429824


2. enough to just use Master Theorem?
critical polynomial is n^(log22) = n = f(n)
so, `θ(n^(logbᵃ)·log2ⁿ)` 
so, `θ(n^(1)·log2ⁿ)` 

T(n) = aT(n/b) + f(y) is this ok? is overhead not a function of n?
