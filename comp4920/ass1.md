<!-- SPDX-License-Identifier: zlib-acknowledgement -->
TODO: harvard in-text referencing
(Smith, 2020, p. 15).
(Jones, 2018, pp. 22-23).

if only referencing in-text sources, i.e. from readings, won't this just be the same throughout
or is referencing something in text of another reference ok?

pg. 79:
Sartre calls “bad faith”: making things
easier for yourself by pretending that you have no choice, when really there are
always options open to you 

pg. 89:
indeed, the critics of
Hume’s view will claim that it is too simple for some of the acknowledged facts about
human beings, and that it is based in a reductive view of how human beings act

pg. 104:
This might lead on to a criticism of virtue ethics: that it
cannot be a comprehensive moral theory because it cannot provide us with
proper answers to the many moral questions in which rights play an important
role

pg. 107:
 On McDowell’s view, “flourishing”
is not itself a substantive idea

- answered the question(s) with reasons
- articulated objections to reasons and responses to these
- specifically cite course material to support points being made
- cite external literature also?

TODO: Regulation: web hosters hosting malicious sites

Daily life continues to involve more technologies.
As a result, there are far-reaching societal impacts to consider when creating computer science ethical guidelines. 
Act utilitarianism focuses on acts that lead to the most overall good, while rule utilitarianism emphasises following rules that lead to the greatest good.
By considering leadership challenges relating to data obtainment and user freedom,
it can be seen that rule utilitarianism is more desirable.
It allows for the creation of guidelines that are both beneficial to society and simple for a governing body to implement.

Under rule utilitarianism, ethical guidelines for data retrieval can be made consistent.
This promotes societal confidence in the computer science profession, 
leading to a happier society.
Decisions made relating to data retrieval practices in technologies greatly impact user's rights.
Github copilot is an AI code completion tool that is trained on public code repositories (Github, 2023).
Considering act utilitarianism, it's permissable for technologies like this
to scrape as much user data as it wants, regardless of user consent.
A justification for this could be that the resulting technology enables many programmers to create impactful software that they otherwise would've lacked the knowledge to do so.
Therefore, the rights of the user(s) who created the code
are supplanted by the prospect of enpowering many others. 
Proponents of this could argue that if consent was required, most likely no one would ever consent
and the benefits of the technology could not be realised.
This echoes the self-defeatist criticism of act utilitarianism discussed by Bennett (2015, pg. 61).
Specifically, that in advancing technology accessibility as a means of improved happiness,
users of technologies lose trust in whether their data is protected.
This distrust can reduce technological adoption, decreasing overall happiness.
Following rule utilitarianism, it can be mandated that AI models should only be trained on data that has user consent.
Importantly, this protects data that predates any current rights framework.
Under this system, users can be confident in how their past and present data will be used.
Critics may argue that this could reduce technological innovation due to limited data for training. 
However, the long-term benefits of fostering a society that trusts tech companies outweigh these short-term costs.
As a result, the consistency that rule utilitarianism offers makes it a more effective approach for promoting trust in the computer science profession.

 this also violates ACM ethical guideline principal that 'All people are stakeholders in computing' 
 which mentions the perogative to "protect each individual's right to autonomy". (ACM, 2018 pg. 4)
 It would seem prudent to adhere to an existing governing body's conclusions that dedicated much resources and manpower
 of knowledgeable people to backup validity.


Following rule utilitarianism, ethical guidelines concerning user freedom can 
data retrieval can be made consistent.
This promotes societal confidence in the computer science profession, 
leading to a happier society.
Decisions made relating to data retrieval practices in technologies greatly impact user's rights.

Intel SGX is a security technology that utilises a hardware private key to allow the CPU to only run signed code (Intel, n.d).
Under act utilitarianism, it's possible for implementors of this technology to effectively brick an individual's machine.
This would be done by blacklisting their private key, meaning they could no longer run any new software.
A justification for this could be that rendering a malicious user offline would safeguard many more users who were potential victims.
Fundamentally, this technology requires that society trusts CPU manufacturers to act justly.
It can be said that, to many, CPUs are already a black box, so society must already effectively trust them regardless if using SGX.

In fact, this could restrict users downloading new software if developer's deemed not wanting to run them for some reason.
positive would be malware users.


However, once a user buys a machine with their own money it becomes their property; is it not considered immoral to restrict their use of it?
They should still be able to use it, just not in that way.
This is an example of act utilitarianism leading to immoral results, as described by Bennett (2015, pg. 60).
This would lead to unhappiness in society.
Comparatively, under rule utilitarianism, can say limit malicious access as long as they retain control of their devices.
Detractors of this could argue that without this ability, assuming unrealistic value-neutrality thesis outlined by Pitt ().
Detractors of this argue that this promotes malware, botnet proliferation...

However, focusing on enhanced security measures like ...
this does in fact more closely align with Kranzberg's laws, acknowledging technology has potential for misuse.

Intel SGX is a security technology that uses a hardware private key, allowing the CPU to run only signed code (Intel, n.d.). Under act utilitarianism, implementers of this technology could effectively render an individual's machine unusable by blacklisting their private key, preventing them from running new software. A potential justification is that disabling a malicious user would protect many more potential victims. This technology fundamentally relies on society trusting CPU manufacturers to act justly. Given that CPUs are often seen as a black box, society already places implicit trust in them, even without SGX. However, once a user purchases a machine, it becomes their property, raising the question: is it not immoral to restrict their use of it? This illustrates how act utilitarianism can lead to immoral outcomes, as discussed by Bennett (2015, p. 60).
This is an example of act utilitarianism leading to immoral results, as described by Bennett (2015, p. 60). Such actions would likely cause widespread unhappiness in society. Comparatively, under rule utilitarianism, the focus would shift to establishing rules that maximize overall well-being, such as ensuring users retain control over their devices, provided they don't engage in harmful activities. Detractors of this approach argue that it could inadvertently promote the spread of malware and the proliferation of botnets, as malicious users would not face immediate consequences like blacklisting. However, proponents suggest that implementing fair, transparent rules—such as enhanced security protocols and accountability measures—would both protect individual rights and minimize the risk of large-scale harm, thus aligning with the broader goal of societal happiness.


value users freedom (intel sgx),
 - immoral results

Indeed following Kranzberg's First Law, 
“Technology is neither good nor bad; nor is it neutral.”
that mentions 
Melvin Kranzberg, “Technology and History: “Kranzberg’s Laws”, Technology and Culture 27,
no. 3 (1986): 544-560.

Value-Neutrality Thesis (VNT): “Technological artifacts do not have, have
embedded in them, or contain values. (Pitt 2000; Pitt 2014)
Pitt J. C. 2014. ““Guns Don’t Kill, People Kill”; Values in and/or around Technologies.” In The
Moral Status of Technical Artifacts, edited by Kroes P., Verbeek P. P., 89–101. Dordrecht, the
Netherlands: Springer.



precautions to prevent reidentification of anonymised data
this in-fact aligns with ACM ethical guidelines

"show is simply that it is not optimal always to follow the rule"

misuse of technology (generative AI audio deepfakes)
 - cumbersome method (e.g. ok if person can no longer talk etc.)


## Reference List
1. GitHub (2023). GitHub Copilot · Your AI pair programmer. 
   [online] Available at: https://github.com/features/copilot
   [Accessed 18 Sep. 2024].
2. Bennet, C. (2015): "What is this thing called ethics", second edition, Routledge.
3. ACM, 2018. ACM Code of Ethics and Professional Conduct. New York: Association for Computing Machinery, Inc. DOI: 10.1145/3274591.
4. Intel. (n.d.). Intel® Software Guard Extensions (Intel® SGX). 
   [online] Available at: https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/software-guard-extensions.html
   [Accessed 18 Sep. 2024].

rule:
consistency (build trust as all computer scientists follow same principles),
scalability (technology continues to grow and touch many aspects of life),
simplicity (governing bodies to enforce)
