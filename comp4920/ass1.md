<!-- SPDX-License-Identifier: zlib-acknowledgement -->
TODO: harvard in-text referencing
(Smith, 2020, p. 15).
(Jones, 2018, pp. 22-23).

if only referencing in-text sources, i.e. from readings, won't this just be the same throughout
or is referencing something in text of another reference ok?

pg. 79:
Sartre calls “bad faith”: making things
easier for yourself by pretending that you have no choice, when really there are
always options open to you 

pg. 89:
indeed, the critics of
Hume’s view will claim that it is too simple for some of the acknowledged facts about
human beings, and that it is based in a reductive view of how human beings act

pg. 104:
This might lead on to a criticism of virtue ethics: that it
cannot be a comprehensive moral theory because it cannot provide us with
proper answers to the many moral questions in which rights play an important
role

pg. 107:
 On McDowell’s view, “flourishing”
is not itself a substantive idea

- answered the question(s) with reasons
- articulated objections to reasons and responses to these
- specifically cite course material to support points being made
- cite external literature also?

Daily life continues to involve more technologies.
As a result, there are far-reaching societal impacts to consider when creating computer science ethical guidelines based on act versus rule utilitarianism.
By considering leadership challenges relating to data obtainment, technology misuse and user freedom,
it can be seen that rule utilitarianism is more desirable.
It allows for the creation of guidelines that are both beneficial to society and simple for a governing body to implement.

Under rule utilitarianism, ethical guidelines for data retrieval can be made consistent.
This promotes societal confidence in the computer science profession, 
leading to a happier society.
Decisions made relating to data retrieval practices in technologies greatly impact user's rights.
Github copilot (ref.) is an AI code completion tool that is trained on public code repositories.
Considering act utilitarianism, it's conceivable to make it permissable for technologies like this
to scrape as much user data as it wants, regardless of user consent.
A possible justification for this could be that the resulting technology would make programming
accessible to millions more people. Therefore, the rights of the sole user who created the code
are supplanted by the prospect of enpowering many others. 
Proponents of this could argue that if consent was required, most likely no one would ever consent
and the benefits of the technology could not be realised.
This touches on the self-defeatist principle Mark Howell touches on ().
Specifically, that in advancing technology accessibility as a means of improved happiness,
users of technologies lose trust in whether their data is protected, 
reducing adoption of technology and therefore reducing happiness.
Under rule utilitarianism, only train on user consent.
Importantly, this protects data previously existing that doesn't have any 'rights' for them.
Under this framework, user's have the confidence in how their previous and current data
will be used, regardless of regime change in the future.
Like Graham Hins mentions (), this ...
Whilst this does have the possible drawback of ...
While this may limit some potential technological innovations in the short term, 
rule utilitarianism would contend that the long-term benefits to society of having ethical norms that people can rely on outweigh the costs.
Widespread loss of faith in tech companies' handling of personal data could lead to reduced adoption of beneficial technologies, calls for overly restrictive regulation, and a general atmosphere of suspicion that hinders technological progress and social cooperation. Additionally, having clear ethical guidelines provides a framework for developing technologies in a responsible manner that respects individual rights while still pursuing innovation.


obtainment of data to train (github copilot)
act: as benefits more people to use techonology with less skill, do it 
rule: don't force creators to have their data be used for training; be explicit about it

"show is simply that it is not optimal always to follow the rule"

misuse of technology (generative AI audio deepfakes)
 - cumbersome method (e.g. ok if person can no longer talk etc.)

value users freedom (intel sgx),
 - immoral results


rule:
consistency (build trust as all computer scientists follow same principles),
scalability (technology continues to grow and touch many aspects of life),
simplicity (governing bodies to enforce)
