<!-- SPDX-License-Identifier: zlib-acknowledgement -->
TODO: harvard in-text referencing
(Smith, 2020, p. 15).
(Jones, 2018, pp. 22-23).

if only referencing in-text sources, i.e. from readings, won't this just be the same throughout
or is referencing something in text of another reference ok?

pg. 79:
Sartre calls “bad faith”: making things
easier for yourself by pretending that you have no choice, when really there are
always options open to you 

pg. 89:
indeed, the critics of
Hume’s view will claim that it is too simple for some of the acknowledged facts about
human beings, and that it is based in a reductive view of how human beings act

pg. 104:
This might lead on to a criticism of virtue ethics: that it
cannot be a comprehensive moral theory because it cannot provide us with
proper answers to the many moral questions in which rights play an important
role

pg. 107:
 On McDowell’s view, “flourishing”
is not itself a substantive idea

- answered the question(s) with reasons
- articulated objections to reasons and responses to these
- specifically cite course material to support points being made
- cite external literature also?

TODO: Regulation: web hosters hosting malicious sites

Daily life continues to involve more technologies.
As a result, there are far-reaching societal impacts to consider when creating computer science ethical guidelines based on act versus rule utilitarianism.
By considering leadership challenges relating to data obtainment, technology misuse and user freedom,
it can be seen that rule utilitarianism is more desirable.
It allows for the creation of guidelines that are both beneficial to society and simple for a governing body to implement.

Under rule utilitarianism, ethical guidelines for data retrieval can be made consistent.
This promotes societal confidence in the computer science profession, 
leading to a happier society.
Decisions made relating to data retrieval practices in technologies greatly impact user's rights.
Github copilot is an AI code completion tool that is trained on public code repositories (Github, 2023).
Considering act utilitarianism, it's permissable for technologies like this
to scrape as much user data as it wants, regardless of user consent.
A justification for this could be that the resulting technology enables many programmers to create impactful software that they otherwise would've lacked the knowledge to do so.
Therefore, the rights of the user(s) who created the code
are supplanted by the prospect of enpowering many others. 
Proponents of this could argue that if consent was required, most likely no one would ever consent
and the benefits of the technology could not be realised.
This echoes the self-defeatist criticism of act utilitarianism discussed by Bennett (2015, pg. 61).
Specifically, that in advancing technology accessibility as a means of improved happiness,
users of technologies lose trust in whether their data is protected.
This distrust can reduce technological adoption, decreasing overall happiness.
Following rule utilitarianism, it can be mandated that AI models should only be trained on data that has user consent.
Importantly, this protects data that predates any current rights framework.
Under this system, users can be confident in how their past and present data will be used.
Critics may argue that this could reduce technological innovation due to limited data for training. 
However, the long-term benefits of fostering a society that trusts tech companies outweigh these short-term costs.
As a result, the consistency that rule utilitarianism offers makes it a more effective approach for promoting trust in the computer science profession.

value users freedom (intel sgx),
 - immoral results
 technology currently only available to data centres
 SGX requires that you trust Intel
 arguments may be made that using any CPU manufacturer you must trust them, regardless if using SGX or not, so point is moot

 example of utilitarianism leading to immoral results Bennett (2015, pg. 60).
 this also violates ACM ethical guideline principal that 'All people are stakeholders in computing' 
 which mentions the perogative to "protect each individual's right to autonomy". (ACM, 2018 pg. 4)
 It would seem prudent to adhere to an existing governing body's conclusions that dedicated much resources and manpower
 of knowledgeable people to backup validity.



precautions to prevent reidentification of anonymised data
this in-fact aligns with ACM ethical guidelines

"show is simply that it is not optimal always to follow the rule"

misuse of technology (generative AI audio deepfakes)
 - cumbersome method (e.g. ok if person can no longer talk etc.)


## Reference List
1. GitHub (2023). GitHub Copilot · Your AI pair programmer. 
   [online] Available at: https://github.com/features/copilot
   [Accessed 18 Sep. 2024].
2. Bennet, C. (2015): "What is this thing called ethics", second edition, Routledge.
3. ACM, 2018. ACM Code of Ethics and Professional Conduct. New York: Association for Computing Machinery, Inc. DOI: 10.1145/3274591.
4. Intel. (n.d.). Intel® Software Guard Extensions (Intel® SGX). 
   [online] Available at: https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/software-guard-extensions.html
   [Accessed 18 Sep. 2024].

rule:
consistency (build trust as all computer scientists follow same principles),
scalability (technology continues to grow and touch many aspects of life),
simplicity (governing bodies to enforce)
