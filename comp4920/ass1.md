<!-- SPDX-License-Identifier: zlib-acknowledgement -->
TODO: harvard in-text referencing
(Smith, 2020, p. 15).
(Jones, 2018, pp. 22-23).

- answered the question(s) with reasons
- articulated objections to reasons and responses to these
- specifically cite course material to support points being made
- cite external literature also?

Daily life continues to involve more technologies.
As a result, there are far-reaching societal impacts to consider 
when creating computer science ethical guidelines. 
Act utilitarianism focuses on acts that lead to the most overall good, 
while rule utilitarianism emphasises following rules that lead to the greatest good.
By considering leadership challenges relating to data obtainment and user security,
it can be seen that rule utilitarianism is more desirable.
It allows for the creation of guidelines that 
are both beneficial to society and simple for a governing body to implement.

TODO: consent and only used for what saying
TODO: don't make consent essential for application use 
(detractors might say introduce biases by limiting data as in reading ...) 

Under rule utilitarianism, ethical guidelines for data retrieval can be made consistent.
This promotes societal confidence in the computer science profession, 
leading to a happier society.
Decisions made relating to data retrieval practices in technologies 
greatly impact user's rights.
Github copilot is an AI code completion tool that is trained on 
public code repositories (Github, 2023).
Considering act utilitarianism, it's permissable for technologies like this
to scrape as much user data as desired and use it for any purpose. 
These actions can be done regardless of user consent.
A justification for this could be that the resulting technology enables many 
programmers to create impactful software that they otherwise would've lacked 
the knowledge to do so.
Therefore, the rights of the user(s) who created the code
are supplanted by the prospect of enpowering many others. 
Proponents of this could argue that if consent was required, 
most likely no one would ever consent and the benefits of the technology could not be realised.
This echoes the self-defeatist criticism 
of act utilitarianism discussed by Bennett (2015, p. 61).
Specifically, that in advancing technology accessibility as a means of improved happiness,
users of technologies lose trust in whether their data is protected.
This distrust can reduce technological adoption, decreasing overall happiness.
Following rule utilitarianism, it can be mandated that 
data collection should only occur with explicit user consent
and clear specification of how the data will be used.
Importantly, this protects data that predates any current rights framework.
Such protection goes beyond a value neutral approach to technology ().
It acknowledges Kranzberg's First Law that 
the future uses of technology cannot be fully foreseen by its creators ().
By requiring both consent and specified use, 
this system provides users with confidence in how their past, present, 
and future data will be handled. 
Critics may argue that this could reduce technological innovation and 
introduce biases due to limited data for training (bias ref).
However, the long-term benefits of fostering a society that trusts tech companies 
outweigh these short-term costs.
Indeed, preserving trust aligns with Friedman's notion that
all computer technology should reflect human values to promote a better future (Friedman).
As a result, the consistency that rule utilitarianism offers in relation to data privacy
makes it a more effective approach for developing ethical guidelines than
act utilitarianism.

TODO: change to act in accordance with laws if none exist for technology, create
In the framework of rule utilitarianism, ethical guidelines concerning user security can be structured to protect individual rights at all times.
This provides society with a confidence of autonomy in the technology they use, enhancing happiness.
Security technologies introduce significant power dynamics between producers and consumers.
Intel SGX is a security technology that utilises a hardware private key to allow the CPU to only run signed code (Intel, n.d).
Under act utilitarianism, implementers of this technology could effectively render a user's machine unusable.
This would be done by blacklisting their private key, preventing them from running new software. 
A justification for this could be that rendering a malicious user offline would safeguard many more users who were potential victims.
As a result, that user's automony over their machine is forfeited for the general good.
This outcome is in violation of the ACM ethical guideline principal concerning people as stakeholders in computing.
Specifically, the imperative that all computing should protect each individual's right to autonomy (ACM, 2018 p. 4).
Having this as a possible outcome relies on society placing an immense amount of trust in technology manufactuers to act justly.
This required faith could lead to societal dissatisfaction.
Some may argue that law enforcement already has the power to seize personal property, and that allowing educated tech companies to take over this role could expedite processes and reduce bureaucracy.
However, the key issue is that technology companies would have unchecked control over user's belongings, effectively placing them above the law. 
With the rise of smart devices, this control could extend to phones, appliances, cars, and more.
This illustrates how act utilitarianism can lead to immoral outcomes, as discussed by Bennett (2015, p. 60).
It's immoral to seize one's property without proper litigation/warrant.
In contrast, rule utilitarianism would establish guidelines stating that security technologies restricting user freedom 
should only be implemented through legal channels, such as law enforcement.
Critics argue that this approach could inadvertently allow the spread of malware and botnets, as malicious users wouldn't face immediate consequences like blacklisting. 
However, proponents argue that transparent, fair rules—such as enhanced security protocols and accountability measures—would protect individual rights and reduce the risk of large-scale harm, aligning with the goal of societal happiness.


In contrast, rule utilitarianism would establish guidelines stating that security technologies restricting user freedom should only be implemented through legal channels, such as law enforcement. Moreover, under rule utilitarianism, if no law exists to cover specific actions like remotely disabling a user's device, the ethical imperative would be to create appropriate legislation before releasing such technology. This proactive approach ensures that the legal framework keeps pace with technological advancements, maintaining a balance between innovation and individual rights. By doing so, it prevents the potential misuse of power by technology companies and ensures that any actions taken against users' devices are subject to proper legal scrutiny and oversight. This approach aligns with the broader goal of maximizing societal happiness by providing clear, transparent rules that protect individual autonomy while still allowing for necessary security measures.
Critics argue that this approach could inadvertently slow down technological progress or allow the spread of malware and botnets, as malicious users wouldn't face immediate consequences like blacklisting without established legal procedures. However, proponents argue that the benefits of having a comprehensive legal framework in place before deploying potentially restrictive technologies far outweigh these concerns. They contend that transparent, fair rules—such as enhanced security protocols, accountability measures, and legal safeguards—would protect individual rights and reduce the risk of large-scale harm, ultimately contributing to greater societal trust and happiness.

In conclusion, ethical guidelines for the computer science profession should prioritise societal wellbeing. 
By examining leadership challenges in data privacy and user security, 
it is evident that rule utilitarianism better serves this goal compared to act utilitarianism. 
It does so by promoting consistent and straightforward rules that are easier to implement.


## Reference List
1. GitHub (2023). GitHub Copilot · Your AI pair programmer. 
   [online] Available at: https://github.com/features/copilot
   [Accessed 18 Sep. 2024].
2. Bennet, C. (2015): "What is this thing called ethics", second edition, Routledge.
3. ACM, 2018. ACM Code of Ethics and Professional Conduct. New York: Association for Computing Machinery, Inc. DOI: 10.1145/3274591.
4. Intel. (n.d.). Intel® Software Guard Extensions (Intel® SGX). 
   [online] Available at: https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/software-guard-extensions.html
   [Accessed 18 Sep. 2024].

Value-Neutrality Thesis (VNT): “Technological artifacts do not have, have
embedded in them, or contain values. (Pitt 2000; Pitt 2014)
Pitt J. C. 2014. ““Guns Don’t Kill, People Kill”; Values in and/or around Technologies.” In The
Moral Status of Technical Artifacts, edited by Kroes P., Verbeek P. P., 89–101. Dordrecht, the
Netherlands: Springer.

Melvin Kranzberg, “Technology and History: “Kranzberg’s Laws”, Technology and Culture 27,
no. 3 (1986): 544-560.
