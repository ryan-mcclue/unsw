#!/usr/bin/python3
# SPDX-License-Identifier: zlib-acknowledgement

# don't have enough training images for neural network?

# train/train/image_id_001.jpg (for supervised learning)
#  {
#    "id": 0,
#    "image_id": 0,
#    "category_id": 1,
#    "bbox": [
#      119,
#      25,
#      205,
#      606
#    ],
#    "area": 124230,
#    "segmentation": [],
#    "iscrowd": 0
#  },


# connected_components (bounding boxes) -> rst_feature_extraction 

# compute euclidean distance between features (minimum distance classifier)

# 1. establish features from labelled set that identify classes (RST_invariant features?)
# 2. compare these against all features in 

# (similarity of features) Euclidean distance, Manhattan distance, or cosine similarity, minkowski distance, hamming distance, jaccard similarity -> 
# (decision options) Single Nearest Neighbor, k-Nearest Neighbors, Weighted Voting

# genetic algorithm to generate different solutions based on different similarity and decision strategies for nearest neighbour?
# TODO: (how different to ensemble?) Ensemble methods combine predictions from multiple classifiers to make the final decision. 
# For example, you can train several minimum distance classifiers with different feature representations or distance metrics. 
# During inference, let each classifier independently classify the new image, and the final prediction can be determined through majority voting or averaging the probabilities/confidences from different classifiers
# this voting would be an example of (bagging, i.e. random forests as oppose to boosting) 

# train genetic model based on ensemble learning? or not really applicable as we know what optimal is and could just check all combinations?
# 1. SAVE feature extractors on valid images (RST, HOG, SIFT, SURF, etc.)
# 2. SAVE preprocessed training images
# 3. example genetic solution [feature-extractor, distance-metric, decision-option]
# could use genetic solution to optimise hyperparameters of ensemble learning? 

# https://towardsdatascience.com/genetic-programming-for-image-classification-a505950c5467

def optimise_this(x, y, z):
  return 6**x + y*10-111 * z**z

def fitness(x, y, z):
  ans = optimise_this(x, y, z)

  if ans == 0:
    return large_number # higher the number the better
  else:
    return abs(1/ans)

# generate solutions
solutions = []
for s in range(1000): # is this size of population?
  solutions.append((random.uniform(0, 10000), random.uniform(0, 10000), random.uniform(0, 10000)))

# create generations
for i in range(num_generations):

  ranked_sol = []
  for s in solutions:
    ranked_sol.append((s, fitness(s)))
  ranked_sol.sort()
  ranked_sol.reverse()

  # IMPORTANT(Ryan): Could break if say find solution with > 0.9 accuracy?

  best_sol = ranked_sol[:100]

  elements = [] 
  for s in best_sol:
    elements.append(s)

  new_gen = []
  for _ in range(1000):
    e1 = random.choice(elements) * random.uniform(0.99, 1.01) 
    e2 = random.choice(elements) * random.uniform(0.99, 1.01) 
    e3 = random.choice(elements) * random.uniform(0.99, 1.01) 
    new_gen.append((e1, e2, e3))

  ranked_sol = new_gen

import random

# Genetic Algorithm Parameters
population_size = 100
mutation_rate = 0.1
crossover_rate = 0.8
num_generations = 100

# Define your problem-specific functions and parameters

def initialize_population():
    # Initialize a population of random individuals
    population = []
    for _ in range(population_size):
        individual = generate_individual()  # Generate a random individual
        population.append(individual)
    return population

def generate_individual():
    # Generate a random individual
    # Define the representation of your individuals based on your problem
    # Return a randomly generated individual

def evaluate_fitness(individual):
    # Evaluate the fitness of an individual based on your problem-specific criteria
    # Return a fitness value indicating the quality of the individual

def selection(population):
    # Apply selection to choose individuals for mating
    # Implement your selection strategy (e.g., tournament selection, roulette wheel selection)
    # Return a list of selected individuals

def crossover(parent1, parent2):
    # Perform crossover between two parents to produce offspring
    # Implement your crossover strategy (e.g., one-point crossover, two-point crossover)
    # Return the offspring generated by crossover

def mutate(individual):
    # Apply mutation to an individual
    # Implement your mutation strategy (e.g., randomly flip bits, randomly perturb values)
    # Return the mutated individual

# Main Genetic Algorithm Loop
population = initialize_population()

for generation in range(num_generations):
    # Evaluate fitness of each individual
    fitness_scores = [evaluate_fitness(individual) for individual in population]

    # Perform selection to choose parents
    selected_parents = selection(population)

    # Create the next generation through crossover and mutation
    next_generation = []
    while len(next_generation) < population_size:
        parent1 = random.choice(selected_parents)
        parent2 = random.choice(selected_parents)
        if random.random() < crossover_rate:
            offspring = crossover(parent1, parent2)
        else:
            offspring = parent1  # No crossover, offspring is a copy of parent1
        if random.random() < mutation_rate:
            offspring = mutate(offspring)
        next_generation.append(offspring)

    # Replace the current population with the next generation
    population = next_generation

    # Perform any additional analysis or logging at the end of each generation

# End of Genetic Algorithm




# python3 -m venv name 
# source name/bin/activate; deactivate (.gitignore name)
# pip freeze > requirements.txt

# penguins and turtles classifier

# cannot use popular detection or classification methods
# can expand upon them, modify or brand new


# valid/valid/image_id_001.jpg (for testing)
#{
#    "id": 0,
#    "image_id": 0,
#    "category_id": 1,
#    "bbox": [
#      227,
#      93,
#      298,
#      525
#    ],
#    "area": 156450,
#    "segmentation": [],
#    "iscrowd": 0
#  },

# detect or classifier first?

# The presentation must start with an introduction of the problem and then explain the usedmethods, show the obtained results, and discuss these results as well as ideas for future improvements.

# (introduction, methods, results, discussion, demonstration)


# genetic algorithm to optimise cell selection?
# so, say segmentation based on 7 parameters. genetic algorithm to calculate optimum value for these parameters?

# YOLO
# a cell can only contain 1 object (so for more, need finer grid, i.e. higher S value)
# 1. divide img into SxS grid. of these cells, select cell that has objects midpoint. bounding boxes relative to cell. probability of an object in cell
